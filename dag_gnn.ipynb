{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUu8dBxf_l7_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgR82ZRGhU_6"
   },
   "outputs": [],
   "source": [
    "def identity_transpose(A):\n",
    "    '''Calculate (I - A^T)'''\n",
    "    return tf.eye(A.shape[0], A.shape[0]) - tf.transpose(A)\n",
    "\n",
    "def identity_transpose_inverse(A):\n",
    "    '''Calculate (I - A^T)^(-1)'''\n",
    "    return tf.linalg.inv(identity_transpose(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GeHoXOg2DzRU"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder class for DAG-GNN method\n",
    "\n",
    "    Inputs:\n",
    "    adjA (tensor [d, d]) : current estimated adjascency matrix\n",
    "    ind_dim (int) : dimension of input layer\n",
    "    hid_dim (int) : dimension of hidden layer\n",
    "    out_dim (int) : dimension of output layer\n",
    "\n",
    "    Outputs:\n",
    "    out (tensor [batch, d]) : output of neural network\n",
    "    ligs (tensor [d, d]) : product of (I - A^T @ out)\n",
    "    adjA (tensor [d, d]) : current estimated adjascency matrix\n",
    "\n",
    "    '''\n",
    "    def __init__(self, adjA, in_dim, hid_dim, out_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.adjA = tf.Variable(initial_value = adjA, trainable = True)\n",
    "        #self.Wa = tf.Variable(np.zeros(), trainable = True)\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(hid_dim, activation= 'relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(out_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''Forward process of neural network'''\n",
    "        #calculate I - A^T\n",
    "        I_adjA = identity_transpose(self.adjA)\n",
    "        hidden = self.fc1(inputs)\n",
    "        outputs = self.fc2(hidden)\n",
    "        logits = tf.matmul(I_adjA, outputs)\n",
    "        return outputs, logits, self.adjA\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    '''\n",
    "    Decoder class for DAG-GNN method\n",
    "\n",
    "    Inputs:\n",
    "    ind_dim (int) : dimension of input layer\n",
    "    out_dim (int) : dimension of output layer\n",
    "    hid_dim (int) : dimension of hidden layer\n",
    "\n",
    "    Outputs:\n",
    "    '''\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(hid_dim, activation = 'relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(out_dim)\n",
    "\n",
    "    def call(self, z_inputs,  adjA):\n",
    "\n",
    "        #calculate (I - A^T)^(-1)\n",
    "        I_adjA = identity_transpose(adjA)\n",
    "        z = tf.matmul(I_adjA, z_inputs)\n",
    "\n",
    "        hidden = self.fc1(z)\n",
    "        outputs = self.fc2(hidden)\n",
    "        return z, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAG_GNN_VAE(tf.keras.Model):\n",
    "    '''\n",
    "    Model class for DAG-GNN method training\n",
    "\n",
    "    Inputs:\n",
    "    adjA (tensor [d, d]) : current estimated adjascency matrix\n",
    "    ind_dim (int) : dimension of input layer\n",
    "    hid_dim (int) : dimension of hidden layer\n",
    "    out_dim (int) : dimension of output layer\n",
    "\n",
    "    Outputs:\n",
    "    out (tensor [batch, d]) : output of neural network\n",
    "    ligs (tensor [d, d]) : product of (I - A^T @ out)\n",
    "    adjA (tensor [d, d]) : current estimated adjascency matrix\n",
    "\n",
    "    '''\n",
    "    def __init__(self, adjA, in_dim, hid_dim, out_dim):\n",
    "        super(DAG_GNN_VAE, self).__init__()\n",
    "        self.encoder = Encoder(adjA, in_dim, hid_dim, out_dim)\n",
    "        self.decoder = Decoder(in_dim, hid_dim, out_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        en_outputs, logits, new_adjA = self.encoder(inputs)\n",
    "        z, de_outputs = self.decoder(logits, new_adjA)\n",
    "        return en_outputs, logits, new_adjA, z, de_outputs\n",
    "        \n",
    "    def _h(A):\n",
    "        '''Calculate the constraint of A ensure that it's a DAG'''\n",
    "        #(Yu et al. 2019 DAG-GNN)\n",
    "        # h(w) = tr[(I + kA*A)^n_variables] - n_variables\n",
    "        M = tf.eye(n_variables, num_columns = n_variables) + A/n_variables\n",
    "        E = M\n",
    "        for _ in range(n_variables - 2):\n",
    "            E = tf.linalg.matmul(E, M)\n",
    "        h = tf.math.reduce_sum(tf.transpose(E) * M) - n_variables\n",
    "        return h\n",
    "    \n",
    "    def _loss(A, logits, X, Y):\n",
    "        '''\n",
    "        Function that evaluate the model loss\n",
    "        loss = kl loss + nll loss + dag constraint + l1 reg + l2 reg\n",
    "        '''\n",
    "        # h constraint loss\n",
    "        h = self._h(A)\n",
    "        h_loss = 0.5 * rho * h * h + alpha * h\n",
    "        \n",
    "        #KL divergence\n",
    "        kl_loss = tf.sum(tf.pow(logits, 2) / ( 2 * logits.shape[0]))\n",
    "        \n",
    "        #negative likelihood loss\n",
    "        nll_loss = tf.sum(tf.pow(X - Y, 2) / (2 * n_variables))\n",
    "        \n",
    "        #L1 penalization\n",
    "        l1_loss = lambda1 * tf.sum(tf.abs(A))\n",
    "        \n",
    "        #diagonal penalization\n",
    "        diag_loss = 100 * tf.linalg.trace(A * A)\n",
    "        \n",
    "        loss = h_loss + kl_loss + nll_loss + l1_loss + diag_loss\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.ones((5, 5)).astype(np.float32)\n",
    "data = np.ones((5, 200)).astype(np.float32)\n",
    "model = DAG_GNN_VAE(A, 5, 20, 5)\n",
    "model.build(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1X2jo0-Szr61"
   },
   "outputs": [],
   "source": [
    "def dag_gnn(data, hid_dim = 20, h_tol = 1e-8, threshold = 0.3, lambda1 = 0.1, rho_max = 10e20, max_iter = 10e8, n_epochs = 20):\n",
    "    '''\n",
    "    Function that apply the DAG GNN method to estimate a DAG\n",
    "    \n",
    "    Inputs:\n",
    "        data (numpy.matrix) : [n_samples, n_variables] samples matrix \n",
    "        hid_dim  (int) : list of dimensions for neural network hidden layer\n",
    "        h_tol (float) : tolerance for constraint, exit condition \n",
    "        threshold (float) : threshold for W_est edge values\n",
    "        lambda1 (float) : L1 regularization parameter\n",
    "        rho_max (float) : max value for rho in augmented lagrangian\n",
    "        max_iter (int) : max number of iterations\n",
    "        n_epochs (int) : number of epochs\n",
    "    Outputs:\n",
    "        A_est (numpy.matrix): [n_variables, n_variables] estimated graph\n",
    "    '''\n",
    "    \n",
    "    def update_optmizer(optimizer, rho):\n",
    "        '''related LR to rho, whenever rho gets big, reduce LR proportionally'''\n",
    "        MAX_LR = 1e-2\n",
    "        MIN_LR = 1e-4\n",
    "        \n",
    "        new_lr = / (np.log10(rho) + 1e-10)\n",
    "        lr = min(MAX_LR, max(MIN_LR, new_lr)) #if new_lr is inside limits, use it\n",
    "        \n",
    "        #update LR\n",
    "        \n",
    "        return optmizer, lr\n",
    "    \n",
    "    def train(optimizer):\n",
    "        '''Model training'''\n",
    "        #update optmizer\n",
    "        \n",
    "        optimizer, lr = update_optmizer(optimizer, rho)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_id, batch_data in enumerate(train_loader):\n",
    "                \n",
    "                #passing through neural network\n",
    "                en_outputs, logits, adjA, z, de_outputs = vae(batch_data)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(vae.trainable_variables)\n",
    "                    #calculate loss\n",
    "                    loss = vae._loss(adjA, logits, decoder_out, batch_data)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        return adjA\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    # Optimization process #\n",
    "    ########################\n",
    "        \n",
    "    n_variables = data.shape[1]\n",
    "    rho, alpha, h = 1., 0., np.Inf\n",
    "    \n",
    "    train_loader, test_loader = setup_data_loader(data)\n",
    "    \n",
    "    #setup of neural networks\n",
    "    new_adj = np.zeros((n_variables, n_variables))\n",
    "    vae = DAG_GNN_VAE(new_adj, n_variables, hid_dim, n_variables)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    \n",
    "    for _ in range(int(max_iter)):\n",
    "        h_new = None\n",
    "        while rho < rho_max:\n",
    "            A_est = train() \n",
    "            h_new = _h(A_est)\n",
    "                \n",
    "            #Update constraint parameter rho\n",
    "            if h_new > 0.25 * h:\n",
    "                rho = rho*10\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        #Ascent alpha\n",
    "        h = h_new    \n",
    "        alpha += rho * h\n",
    "        \n",
    "        #Verifyng constraint tolerance\n",
    "        if h <= h_tol or rho >= rho_max:\n",
    "            break\n",
    "    \n",
    "    #Applyng threshold\n",
    "    A_est[A_est < A_threshold] = 0\n",
    "    return A_est\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dag-gnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
