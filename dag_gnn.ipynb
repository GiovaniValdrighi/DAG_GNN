{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUu8dBxf_l7_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgR82ZRGhU_6"
   },
   "outputs": [],
   "source": [
    "def identity_transpose(A):\n",
    "    '''Calculate (I - A^T)'''\n",
    "    return tf.eye(A.shape[0], A.shape[0]) - tf.transpose(A)\n",
    "\n",
    "def identity_transpose_inverse(A):\n",
    "    '''Calculate (I - A^T)^(-1)'''\n",
    "    return tf.linalg.inv(identity_transpose(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GeHoXOg2DzRU"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder class for DAG-GNN method\n",
    "\n",
    "    Inputs:\n",
    "    adjA (tensor [d, d]) : current estimated adjascency matrix\n",
    "    ind_dim (int) : dimension of input layer\n",
    "    hid_dim (int) : dimension of hidden layer\n",
    "    out_dim (int) : dimension of output layer\n",
    "\n",
    "    Outputs:\n",
    "    out (tensor [batch, d]) : output of neural network\n",
    "    ligs (tensor [d, d]) : product of (I - A^T @ out)\n",
    "    adjA (tensor [d, d]) : current estimated adjascency matrix\n",
    "\n",
    "    '''\n",
    "    def __init__(self, adjA, in_dim, hid_dim, out_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.adjA = tf.Variable(initial_value = adjA, trainable = True)\n",
    "        #self.Wa = tf.Variable(np.zeros(), trainable = True)\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(hid_dim, activation= 'relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(out_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''Forward process of neural network'''\n",
    "        #calculate I - A^T\n",
    "        I_adjA = identity_transpose(self.adjA)\n",
    "        hidden = self.fc1(inputs)\n",
    "        outputs = self.fc2(hidden)\n",
    "        logits = tf.matmul(I_adjA, outputs)\n",
    "        return outputs, logits, self.adjA\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    '''\n",
    "    Decoder class for DAG-GNN method\n",
    "\n",
    "    Inputs:\n",
    "    ind_dim (int) : dimension of input layer\n",
    "    out_dim (int) : dimension of output layer\n",
    "    hid_dim (int) : dimension of hidden layer\n",
    "\n",
    "    Outputs:\n",
    "    '''\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(hid_dim, activation = 'relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(out_dim)\n",
    "\n",
    "    def call(self, z_inputs,  adjA):\n",
    "\n",
    "        #calculate (I - A^T)^(-1)\n",
    "        I_adjA = identity_transpose(adjA)\n",
    "        z = tf.matmul(I_adjA, z_inputs)\n",
    "\n",
    "        hidden = self.fc1(z)\n",
    "        outputs = self.fc2(hidden)\n",
    "        return z, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAG_GNN_VAE(tf.keras.Model):\n",
    "    '''\n",
    "    Model class for DAG-GNN method training\n",
    "\n",
    "    Inputs:\n",
    "    adjA (tensor [d, d]) : current estimated adjascency matrix\n",
    "    ind_dim (int) : dimension of input layer\n",
    "    hid_dim (int) : dimension of hidden layer\n",
    "    out_dim (int) : dimension of output layer\n",
    "\n",
    "    Outputs:\n",
    "    out (tensor [batch, d]) : output of neural network\n",
    "    ligs (tensor [d, d]) : product of (I - A^T @ out)\n",
    "    adjA (tensor [d, d]) : current estimated adjascency matrix\n",
    "\n",
    "    '''\n",
    "    def __init__(self, adjA, in_dim, hid_dim, out_dim):\n",
    "        super(DAG_GNN_VAE, self).__init__()\n",
    "        self.encoder = Encoder(adjA, in_dim, hid_dim, out_dim)\n",
    "        self.decoder = Decoder(in_dim, hid_dim, out_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        en_outputs, logits, new_adjA = self.encoder(inputs)\n",
    "        z, de_outputs = self.decoder(logits, new_adjA)\n",
    "        return en_outputs, logits, new_adjA, z, de_outputs\n",
    "        \n",
    "    def _h(A):\n",
    "        '''Calculate the constraint of A ensure that it's a DAG'''\n",
    "        #(Yu et al. 2019 DAG-GNN)\n",
    "        # h(w) = tr[(I + kA*A)^n_variables] - n_variables\n",
    "        M = tf.eye(n_variables, num_columns = n_variables) + A/n_variables\n",
    "        E = M\n",
    "        for _ in range(n_variables - 2):\n",
    "            E = tf.linalg.matmul(E, M)\n",
    "        h = tf.math.reduce_sum(tf.transpose(E) * M) - n_variables\n",
    "        return h\n",
    "    \n",
    "    def _loss(A, logits, X, Y):\n",
    "        '''\n",
    "        Function that evaluate the model loss\n",
    "        loss = kl loss + nll loss + dag constraint + l1 reg + l2 reg\n",
    "        '''\n",
    "        # h constraint loss\n",
    "        h = self._h(A)\n",
    "        h_loss = 0.5 * rho * h * h + alpha * h\n",
    "        \n",
    "        #KL divergence\n",
    "        kl_loss = tf.sum(tf.pow(logits, 2) / ( 2 * logits.shape[0]))\n",
    "        \n",
    "        #negative likelihood loss\n",
    "        nll_loss = tf.sum(tf.pow(X - Y, 2) / (2 * n_variables))\n",
    "        \n",
    "        #L1 penalization\n",
    "        l1_loss = lambda1 * tf.sum(tf.abs(A))\n",
    "        \n",
    "        #diagonal penalization\n",
    "        diag_loss = 100 * tf.linalg.trace(A * A)\n",
    "        \n",
    "        loss = h_loss + kl_loss + nll_loss + l1_loss + diag_loss\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DAG_GNN_VAE(A, 5, 20, 5)\n",
    "model.build(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'encoder_22/dense_70/kernel:0' shape=(200, 20) dtype=float32, numpy=\n",
       " array([[ 0.09150031,  0.05518448,  0.15420642, ..., -0.16292752,\n",
       "          0.01129237,  0.10451511],\n",
       "        [-0.0789061 ,  0.02459358, -0.16367133, ...,  0.10761529,\n",
       "         -0.0877905 , -0.13330042],\n",
       "        [ 0.13491488, -0.07995654,  0.11635461, ..., -0.14562812,\n",
       "          0.00401524,  0.09812558],\n",
       "        ...,\n",
       "        [ 0.12280121,  0.15659955, -0.1035981 , ...,  0.11804399,\n",
       "          0.07510434,  0.13293636],\n",
       "        [ 0.13769978, -0.14725462, -0.0804465 , ...,  0.02154158,\n",
       "         -0.03118096,  0.05237089],\n",
       "        [-0.04945815, -0.03308129, -0.01368013, ..., -0.04680222,\n",
       "         -0.13116941, -0.03467706]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_22/dense_70/bias:0' shape=(20,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_22/dense_71/kernel:0' shape=(20, 5) dtype=float32, numpy=\n",
       " array([[ 0.43448612,  0.2718782 ,  0.36033484,  0.05202296,  0.27499244],\n",
       "        [ 0.25982508,  0.38873014,  0.03060111,  0.33835593, -0.06772855],\n",
       "        [-0.35909814,  0.02037027, -0.01069158,  0.29005137, -0.308734  ],\n",
       "        [-0.162945  ,  0.38257322, -0.30119893, -0.18489972,  0.37484565],\n",
       "        [ 0.11081168, -0.34228605,  0.1374779 , -0.33270302, -0.01086015],\n",
       "        [-0.3238625 , -0.1823948 ,  0.08605459,  0.31373772,  0.42790708],\n",
       "        [ 0.26541868,  0.4594364 ,  0.34886894,  0.4608555 , -0.20112601],\n",
       "        [ 0.13534322,  0.10163066,  0.0236825 ,  0.33835116, -0.4429599 ],\n",
       "        [-0.3397676 , -0.13637087,  0.33056554, -0.4333464 , -0.15065324],\n",
       "        [-0.48840195, -0.44037208,  0.15331003, -0.14983612, -0.24468231],\n",
       "        [ 0.29916403,  0.4131463 , -0.470251  , -0.15562627, -0.32160836],\n",
       "        [ 0.07333699,  0.43790582, -0.1470947 ,  0.47420397,  0.07234523],\n",
       "        [-0.28813487,  0.3716146 , -0.3079666 ,  0.45618024,  0.13185886],\n",
       "        [-0.36294717,  0.27715352,  0.03531042,  0.45718274, -0.22248712],\n",
       "        [ 0.45358154, -0.37220457, -0.44569305,  0.23843208, -0.13454351],\n",
       "        [-0.02463213, -0.31065303, -0.2599185 , -0.00980064,  0.05300429],\n",
       "        [ 0.00695035, -0.31129262,  0.3739477 ,  0.3808929 , -0.23282856],\n",
       "        [ 0.26114914,  0.35897806, -0.3420791 ,  0.35848585,  0.16882196],\n",
       "        [-0.48357153, -0.36779138, -0.16168356, -0.03440741,  0.09893367],\n",
       "        [ 0.34780362, -0.11537498, -0.0875361 , -0.17459217, -0.06196094]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'encoder_22/dense_71/bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(5, 5) dtype=float32, numpy=\n",
       " array([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'decoder_14/dense_72/kernel:0' shape=(5, 20) dtype=float32, numpy=\n",
       " array([[-0.39351675,  0.3026261 ,  0.26447383, -0.32104713,  0.15414849,\n",
       "          0.13568327,  0.27314535,  0.18888131, -0.1111016 ,  0.3311769 ,\n",
       "          0.21236095,  0.0102571 , -0.00603545, -0.06862712, -0.20211685,\n",
       "         -0.07844117, -0.18864891,  0.08063725,  0.13283369, -0.30878317],\n",
       "        [ 0.14383945,  0.41399577,  0.4014661 ,  0.25351688, -0.32389262,\n",
       "         -0.12196335, -0.11094111,  0.47859266, -0.25081247,  0.4633359 ,\n",
       "         -0.32488975, -0.4690232 ,  0.3268498 ,  0.47241822,  0.19821861,\n",
       "          0.27264848, -0.0405933 , -0.07834098, -0.19620287,  0.48068646],\n",
       "        [-0.14606753,  0.43626884, -0.410305  ,  0.07813224, -0.02085933,\n",
       "          0.03229263,  0.02573165,  0.34769407,  0.36802682,  0.17349961,\n",
       "          0.28623113,  0.31446788,  0.05291566, -0.28539976,  0.29777434,\n",
       "         -0.45948324, -0.37733832, -0.15869954, -0.13201186,  0.42853394],\n",
       "        [ 0.27399907, -0.16224948, -0.23864955,  0.3152828 , -0.48707008,\n",
       "          0.02137163,  0.23395589,  0.19355991,  0.4327046 , -0.21267423,\n",
       "          0.3502244 , -0.39597365, -0.06913939,  0.16965815,  0.17146948,\n",
       "          0.15869758, -0.41830724,  0.47218147, -0.17882046,  0.0682306 ],\n",
       "        [ 0.04004619,  0.1397874 ,  0.34217373,  0.06334594, -0.295028  ,\n",
       "          0.18111268,  0.14700308,  0.00765115,  0.3917388 , -0.3323623 ,\n",
       "         -0.03880239, -0.44188172, -0.13505813,  0.28248653, -0.18131661,\n",
       "         -0.3492635 ,  0.2685754 , -0.01380387, -0.0058099 , -0.02170882]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'decoder_14/dense_72/bias:0' shape=(20,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'decoder_14/dense_73/kernel:0' shape=(20, 5) dtype=float32, numpy=\n",
       " array([[-0.26702082, -0.28119576, -0.04237017,  0.1843442 , -0.13265273],\n",
       "        [-0.3911367 , -0.47775146, -0.02033162, -0.28075004, -0.05561283],\n",
       "        [ 0.37814096, -0.18465185,  0.19813707,  0.33117738,  0.35048047],\n",
       "        [ 0.39984223,  0.2701784 , -0.32559067, -0.16918042, -0.02945983],\n",
       "        [ 0.04805836, -0.05458754,  0.21999833, -0.45271197,  0.3609369 ],\n",
       "        [ 0.39365152, -0.14447787,  0.12963954,  0.478196  ,  0.2882038 ],\n",
       "        [-0.34422776,  0.37783483, -0.3225918 , -0.25910276, -0.11888951],\n",
       "        [-0.1207234 , -0.25970405, -0.01050821,  0.32804444, -0.31158417],\n",
       "        [-0.06411287, -0.18265831, -0.03596517,  0.23510757,  0.48224607],\n",
       "        [ 0.19793525,  0.00822219, -0.09012488, -0.39811286,  0.21193334],\n",
       "        [-0.3628292 ,  0.30371508, -0.16036314, -0.48669678, -0.14267355],\n",
       "        [-0.24170704,  0.30436864, -0.02632993, -0.24935937,  0.4648761 ],\n",
       "        [ 0.18087134,  0.3238695 ,  0.08772555, -0.3219215 ,  0.25842097],\n",
       "        [-0.01310211, -0.31370866,  0.18774757, -0.33493495, -0.1496197 ],\n",
       "        [-0.01774997, -0.24850765,  0.19642314, -0.18538186, -0.19418538],\n",
       "        [ 0.38009235,  0.15704319,  0.33426055, -0.00931707,  0.08672455],\n",
       "        [ 0.4485161 ,  0.28208986,  0.19296083, -0.2543993 ,  0.10741314],\n",
       "        [ 0.348533  ,  0.16672888, -0.06109393,  0.48409632,  0.23768899],\n",
       "        [ 0.1231477 ,  0.13725296,  0.39395127, -0.01066613, -0.4824256 ],\n",
       "        [-0.33469528,  0.21866319,  0.37999895, -0.3787099 ,  0.17571554]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'decoder_14/dense_73/bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       " array([[ 0.89254594, -0.45507213, -0.7312053 , -0.05200586, -0.5441576 ],\n",
       "        [ 0.7041065 ,  0.08252943, -0.48136923,  0.20833716, -0.1983127 ],\n",
       "        [ 0.00293961, -0.10828166, -0.6360076 , -0.00479338, -0.43089932],\n",
       "        [ 0.5052403 , -0.33157814, -0.9160401 , -0.08124065, -0.4729087 ],\n",
       "        [ 0.34194863, -0.24013457, -0.6098543 , -0.18414064, -0.1215876 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       " array([[ 0.89254594, -0.45507213, -0.7312053 , -0.05200586, -0.5441576 ],\n",
       "        [ 0.7041065 ,  0.08252943, -0.48136923,  0.20833716, -0.1983127 ],\n",
       "        [ 0.00293961, -0.10828166, -0.6360076 , -0.00479338, -0.43089932],\n",
       "        [ 0.5052403 , -0.33157814, -0.9160401 , -0.08124065, -0.4729087 ],\n",
       "        [ 0.34194863, -0.24013457, -0.6098543 , -0.18414064, -0.1215876 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(5, 5) dtype=float32, numpy=\n",
       " array([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       " array([[ 0.89254594, -0.45507213, -0.7312053 , -0.05200586, -0.5441576 ],\n",
       "        [ 0.7041065 ,  0.08252943, -0.48136923,  0.20833716, -0.1983127 ],\n",
       "        [ 0.00293961, -0.10828166, -0.6360076 , -0.00479338, -0.43089932],\n",
       "        [ 0.5052403 , -0.33157814, -0.9160401 , -0.08124065, -0.4729087 ],\n",
       "        [ 0.34194863, -0.24013457, -0.6098543 , -0.18414064, -0.1215876 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       " array([[-0.5175284 , -0.37406793, -0.25541598, -0.00338853, -0.03551126],\n",
       "        [-0.21889783, -0.03803962, -0.19279434, -0.13549869,  0.01893594],\n",
       "        [-0.16926864, -0.06564246, -0.16659187,  0.03040989, -0.11804719],\n",
       "        [-0.30408442, -0.28172424, -0.20620668,  0.11368112, -0.11040683],\n",
       "        [-0.18425292, -0.19259788, -0.13250877,  0.03755743, -0.09326056]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1X2jo0-Szr61"
   },
   "outputs": [],
   "source": [
    "def dag_gnn(data, hid_dim = 20, max_iter = 10e8, rho_max = 10e20, n_epochs = 20, lambda1 = 0.1):\n",
    "    '''\n",
    "    Function for inference of DAG with method DAG-GNN\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    def train():\n",
    "        '''Model training'''\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_id, batch_data in enumerate(train_loader):\n",
    "                \n",
    "                #passing through neural network\n",
    "                en_outputs, logits, adjA, z, de_outputs = vae(batch_data)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(vae.trainable_variables)\n",
    "                    #calculate loss\n",
    "                    loss = vae._loss(adjA, logits, decoder_out, batch_data)\n",
    "                \n",
    "                \n",
    "        return adjA\n",
    "    \n",
    "    \n",
    "    #####################\n",
    "    #Augmented lagrangian\n",
    "    #####################\n",
    "        \n",
    "    n_variables = data.shape[1]\n",
    "    rho, alpha, h = 1, 1, np.Inf\n",
    "    \n",
    "    train_loader, test_loader = setup_data_loader(data)\n",
    "    \n",
    "    #setup of neural networks\n",
    "    new_adj = np.zeros((n_variables, n_variables))\n",
    "    vae = DAG_GNN_VAE(new_adj, n_variables, hid_dim, n_variables)\n",
    "    \n",
    "    for _ in range(int(max_iter)):\n",
    "        while rho < rho_max:\n",
    "            A_est = train()\n",
    "            h_new = _h(A_est)\n",
    "                \n",
    "            if h_new > 0.25 * h:\n",
    "                rho = rho*10\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        h = h_new    \n",
    "        alpha += rho * h\n",
    "        \n",
    "        if h <= h_tol:\n",
    "            break\n",
    "    \n",
    "    A_est[A_est < A_threshold] = 0\n",
    "    return A_est\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dag-gnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
